# Databricks notebook source
tables = ['IOT_DIM_CMPD', 
'IOT_DIM_COC_TYPE_APPROVAL', 
'IOT_DIM_COUNTRY', 
'IOT_DIM_CUR', 
'IOT_DIM_DIST', 
'IOT_DIM_DLR', 
'IOT_DIM_DLR_CAR_DELV_ADDRESS', 
'IOT_DIM_EXTR_COLOR', 
'IOT_DIM_INTR_COLOR', 
'IOT_DIM_PRTN', 
'IOT_DIM_SUPPLY_CHAIN_REGION', 
'IOT_DIM_VEH_PRODUCT', 
'IOT_DIM_VEH_PRODUCT_LOCAL', 
'IOT_DIM_VIN', 
'IOT_STG_DLR_DIST_INTERFACE', 
'IOT_STG_EMOT_BTNMPROP', 
'LANDING_ZONE_LEMANS.VPR_SUMMARY', 
'STG.STG_ELM_OUTGOING_MESSAGES', 
'STG_DSR_DIST_CONVERSION', 
'STG_DSR_RSA_SARA_PROVIDERS', 
'STG_DSR_SVC_ITEMS', 
'STG_DSR_SVC_REC_ITEMS', 
'STG_DSR_SVC_RECORDS', 
'STG_DSR_VEHICLE_MASTER', 
'STG_DSR_VEHICLE_WARRANTY', 
'STG_EMOT_BTNADDR', 
'STG_EMOT_BTNBOEVT', 
'STG_EMOT_BTNBPM', 
'STG_EMOT_BTNCOCV', 
'STG_EMOT_BTNCRCAR', 
'STG_EMOT_BTNCUSTCONTRACT', 
'STG_EMOT_BTNCUSTOMER', 
'STG_EMOT_BTNDOC_EVENTS', 
'STG_EMOT_BTNDOC_EVENTS_DETAIL', 
'STG_EMOT_BTNIHDR', 
'STG_EMOT_BTNMUSER', 
'STG_EMOT_BTNOREC', 
'STG_EMOT_BTNORECH', 
'STG_EMOT_BTNPLDEL', 
'STG_EMOT_BTNPPEXT', 
'STG_EMOT_BTNPPLN', 
'STG_EMOT_BTNPPLNH', 
'STG_EMOT_BTNPROP_XREF', 
'STG_EMOT_BTNPROP_XREF_REL', 
'STG_EMOT_BTNREG', 
'STG_EMOT_BTNREGEXDEMO', 
'STG_EMOT_BTNREGEXT', 
'STG_EMOT_BTNREGFR', 
'STG_EMOT_BTNREGH', 
'STG_EMOT_BTNREGSIV', 
'STG_EMOT_BTNRPRT', 
'STG_EMOT_BTNRPRTT', 
'STG_EMOT_BTNVBLFL', 
'STG_EPD_CSTCLSSIFICATIONASSIGN', 
'STG_EPD_CUSTASSIGN', 
'STG_EPD_CUSTCLASSIFICATION', 
'STG_EPD_MODEL', 
'STG_LEM_COMPOUNDS', 
'STG_LEM_COUNTRIES', 
'STG_LEM_CURRENCIES', 
'STG_LEM_DAMAGE_CASES', 
'STG_LEM_DEALERS', 
'STG_LEM_DESTINATIONS', 
'STG_LEM_DISTRIBUTORS', 
'STG_LEM_INCO_TERMS', 
'STG_LEM_LANGUAGES', 
'STG_LEM_LEAD_TIME_TARGETS', 
'STG_LEM_NO_WORKING_DAYS', 
'STG_LEM_ORIGINS', 
'STG_LEM_PARTNER_CODES', 
'STG_LEM_PLANNING_PRIORITIES', 
'STG_LEM_PLANTS', 
'STG_LEM_PORTS', 
'STG_LEM_PROCESSING_TYPES', 
'STG_LEM_ROUTE_DEFS', 
'STG_LEM_ROUTE_SEGMENTS', 
'STG_LEM_ROUTES', 
'STG_LEM_RTDAM_AUDIT', 
'STG_LEM_TRANSPORT_LINE_DEFS', 
'STG_LEM_TRANSPORT_LINES', 
'STG_LEM_TRANSPORT_MODES', 
'STG_LEM_VEH_CMPD_INFORMATION', 
'STG_LEM_VEHICLE_DESTINATIONS', 
'STG_LEM_VEHICLE_DIST_PRICES', 
'STG_LEM_VEHICLE_DISTRIBUTORS', 
'STG_LEM_VEHICLE_ORIGINS', 
'STG_LEM_VEHICLE_ROUTE_SEGMENTS', 
'STG_LEM_VEHICLE_ROUTES', 
'STG_LEM_VEHICLE_STATE_FLOW', 
'STG_LEM_VEHICLE_STATE_TYPES', 
'STG_LEM_VEHICLE_STATE_VALUES', 
'STG_LEM_VEHICLES', 
'STG_LEM_VESSELS', 
'STG_LEM_VOYAGES', 
'STG_LEM_VPR_SUMMARY', 
'STG_LEM_VPR_TYPES', 
'STG_MUD_INTERFACE_CODE', 
'STG_SIEBEL_AGREEMENT_DATA', 
'STG_TECH_DOCUMENT_STATUS', 
'STG_VEH_MASTER_BTV14010', 
'STG_VIN_DSR', 
'STG_VIN_EMOT', 
'STG_VIN_LEM', 
'STG_VIN_LEM_NO_WORKING_DAYS', 
'STG_VIN_WRTY', 
'IOT_STG_DSR_DLR_DIST_LKP', 
'IOT_STG_EMOT_LAST_DEREG', 
'IOT_STG_EMOT_LOAD_RECEIVE', 
'IOT_STG_EMOT_MLE_INVOICE', 
'IOT_STG_EMOT_NSC_INVOICE', 
'IOT_STG_EPD_LPG_CUSTOMIZATION', 
'IOT_STG_LEM_CUSTOMS_STATES', 
'IOT_STG_LEM_DLR_DIST_LKP', 
'IOT_STG_LEM_NO_WORKING_DAYS', 
'IOT_STG_LEM_NO_WORKING_DAYS_TP', 
'IOT_STG_LEM_PROCESS_STATES', 
'IOT_STG_LEM_REPAIR_STATES', 
'IOT_STG_LEM_TRANSPORT_MEXICO', 
'IOT_STG_LEM_TRANSPORT_STATES', 
'IOT_STG_LEM_VPR_SUMMARY', 
'STAGING_TEMP.T$_FIRST_DEMO_OREC', 
'STAGING_TEMP.T$_IOT_DIM_CMPD', 
'STAGING_TEMP.T$_TTL_PPLN', 
'STAGING_TEMP.V$_TEMP_200_ALL_SVC', 
'STAGING_TEMP.V$_TEMP_210_VHM_INFO', 
'STG_DIM_VIN', 
'STG_TMP_LEM_FIRST_TR_UNLOAD', 
'STG_TMP_LEM_PDI_RDY_DATE', 
'STG_VIN_DSR', 
'STG_VIN_EMOT', 
'STG_VIN_LEM', 
'STG_VIN_LEM_NO_WORKING_DAYS', 
'STG_VIN_WRTY'
]
p_scope = "ACC"
p_catalog_name_target = 'impetus_ref'
p_schema_name_source = "STG"
p_table_name_source = 'IOT_DIM_CMPD'

# COMMAND ----------

print([table for table in tables if table.startswith('STG_TMP_')])

# COMMAND ----------

from pyspark.sql.utils import AnalysisException

results = []

try:
    # Check if table exists in Spark catalog
    # spark.table(dbx_qualified_table_name)
    # result = perform_task(catalog_name, schema, table_name)
    result = dbutils.notebook.run("table_load", 3, {"p_scope": p_scope, "p_catalog_name_target": p_catalog_name_target, "p_schema_name_source": p_schema_name_source, "p_table_name_source": p_table_name_source})
    print(result)
    
except AnalysisException as ex:
    # Handle exception if table does not exist
    #print(f"Table {dbx_qualified_table_name} does not exist.")
    dbutils.notebook.exit(str(ex))
# else:
#     dbutils.notebook.exit(f"Table {dbx_qualified_table_name} EXISTS.")
    # # Get schema of table from Spark catalog
    # df = spark.read.table(dbx_qualified_table_name)
    # schema = df.schema
    # print(schema)

# COMMAND ----------

dbutils.notebook.exit({"status" : "OK"})
