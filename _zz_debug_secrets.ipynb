{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create JDBC Scope\n",
    "Create a new JDBC scope using the Databricks secrets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DWH_BI': {'JDBC_URL': 'jdbc:oracle:thin:@//accdw-scan.mle.mazdaeur.com:1521/ACC_DWH',\n",
       "  'JDBC_USERNAME': 'user',\n",
       "  'JDBC_PASSWORD': 'pass',\n",
       "  'JDBC_DRIVER': 'oracle.jdbc.driver.OracleDriver'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "from helpers.env_utils import load_env_as_json, load_env_as_list\n",
    "\n",
    "dev_mode = \"ACP\"\n",
    "\n",
    "secrets :dict = load_env_as_json(dev_mode=dev_mode, env_filename=\".env_init\")\n",
    "display(secrets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = load_env_as_list(dev_mode=dev_mode, env_filename=\".env_init\")\n",
    "display(secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Requirements:\n",
       "- `pip install python-dotenv`\n",
       "- secret scope must have been created\n",
       "- cluster must have been created with secret scope attached\n",
       "- secret scope must have been attached to the cluster\n",
       "- secrets must have been created in the secret scope\n",
       "- secret scope must have been attached to the notebook\n",
       "- secrets must have been attached to the notebook\n",
       "- DB_CREDENTIALS_JSON must have been created in the clusters init script\n",
       "  - `DB_CREDENTIALS_JSON={{secrets/ACP/DWH}}`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%md \n",
    "Requirements:\n",
    "- `pip install python-dotenv`\n",
    "- secret scope must have been created\n",
    "- cluster must have been created with secret scope attached\n",
    "- secret scope must have been attached to the cluster\n",
    "- secrets must have been created in the secret scope\n",
    "- secret scope must have been attached to the notebook\n",
    "- secrets must have been attached to the notebook\n",
    "- DB_CREDENTIALS_JSON must have been created in the clusters init script\n",
    "  - `DB_CREDENTIALS_JSON={{secrets/ACP/DWH}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sergeknecht/Dropbox/Projects/databricks-mazda/_zz_debug_secrets.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sergeknecht/Dropbox/Projects/databricks-mazda/_zz_debug_secrets.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mglobal\u001b[39;00m _sqldf\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sergeknecht/Dropbox/Projects/databricks-mazda/_zz_debug_secrets.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m _sqldf \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49msql(\u001b[39m'''\u001b[39;49m\u001b[39m'''\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sergeknecht/Dropbox/Projects/databricks-mazda/_zz_debug_secrets.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m _sqldf\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/session.py:558\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sqlQuery: \u001b[39mstr\u001b[39m, args: Optional[Union[Dict[\u001b[39mstr\u001b[39m, Any], List]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    557\u001b[0m     cmd \u001b[39m=\u001b[39m SQL(sqlQuery, args)\n\u001b[0;32m--> 558\u001b[0m     data, properties \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mexecute_command(cmd\u001b[39m.\u001b[39;49mcommand(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client))\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msql_command_result\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m properties:\n\u001b[1;32m    560\u001b[0m         \u001b[39mreturn\u001b[39;00m DataFrame\u001b[39m.\u001b[39mwithPlan(CachedRelation(properties[\u001b[39m\"\u001b[39m\u001b[39msql_command_result\u001b[39m\u001b[39m\"\u001b[39m]), \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1046\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     req\u001b[39m.\u001b[39muser_context\u001b[39m.\u001b[39muser_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_user_id\n\u001b[1;32m   1045\u001b[0m req\u001b[39m.\u001b[39mplan\u001b[39m.\u001b[39mcommand\u001b[39m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1046\u001b[0m data, _, _, _, properties \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_and_fetch(req)\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1048\u001b[0m     \u001b[39mreturn\u001b[39;00m (data\u001b[39m.\u001b[39mto_pandas(), properties)\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1351\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, self_destruct)\u001b[0m\n\u001b[1;32m   1348\u001b[0m schema: Optional[StructType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m properties: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1351\u001b[0m \u001b[39mfor\u001b[39;49;00m response \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_and_fetch_as_iterator(req):\n\u001b[1;32m   1352\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(response, StructType):\n\u001b[1;32m   1353\u001b[0m         schema \u001b[39m=\u001b[39;49m response\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1324\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_reattachable_execute:\n\u001b[1;32m   1320\u001b[0m     \u001b[39m# Don't use retryHandler - own retry handling is inside.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     generator \u001b[39m=\u001b[39m ExecutePlanResponseReattachableIterator(\n\u001b[1;32m   1322\u001b[0m         req, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stub, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_policy, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_builder\u001b[39m.\u001b[39mmetadata()\n\u001b[1;32m   1323\u001b[0m     )\n\u001b[0;32m-> 1324\u001b[0m     \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m generator:\n\u001b[1;32m   1325\u001b[0m         \u001b[39myield from\u001b[39;49;00m handle_response(b)\n\u001b[1;32m   1326\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m<frozen _collections_abc>:330\u001b[0m, in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:131\u001b[0m, in \u001b[0;36mExecutePlanResponseReattachableIterator.send\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, value: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pb2\u001b[39m.\u001b[39mExecutePlanResponse:\n\u001b[1;32m    130\u001b[0m     \u001b[39m# will trigger reattach in case the stream completed without result_complete\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_has_next():\n\u001b[1;32m    132\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n\u001b[1;32m    134\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:154\u001b[0m, in \u001b[0;36mExecutePlanResponseReattachableIterator._has_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         \u001b[39mfor\u001b[39;49;00m attempt \u001b[39min\u001b[39;49;00m Retrying(\n\u001b[1;32m    155\u001b[0m             can_retry\u001b[39m=\u001b[39;49mSparkConnectClient\u001b[39m.\u001b[39;49mretry_exception, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_policy\n\u001b[1;32m    156\u001b[0m         ):\n\u001b[1;32m    157\u001b[0m             \u001b[39mwith\u001b[39;49;00m attempt:\n\u001b[1;32m    158\u001b[0m                 \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_current \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n",
      "File \u001b[0;32m~/Dropbox/Projects/databricks-mazda/venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1759\u001b[0m, in \u001b[0;36mRetrying.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1754\u001b[0m             backoff \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m random\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jitter)\n\u001b[1;32m   1756\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   1757\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWill retry call after \u001b[39m\u001b[39m{\u001b[39;00mbackoff\u001b[39m}\u001b[39;00m\u001b[39m ms sleep (error: \u001b[39m\u001b[39m{\u001b[39;00mretry_state\u001b[39m.\u001b[39mexception()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1758\u001b[0m         )\n\u001b[0;32m-> 1759\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sleep(backoff \u001b[39m/\u001b[39;49m \u001b[39m1000.0\u001b[39;49m)\n\u001b[1;32m   1760\u001b[0m     \u001b[39myield\u001b[39;00m AttemptManager(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_retry, retry_state)\n\u001b[1;32m   1762\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m retry_state\u001b[39m.\u001b[39mdone():\n\u001b[1;32m   1763\u001b[0m     \u001b[39m# Exceeded number of retries, throw last exception we had\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%sql\n",
    "SELECT ${session.DB_CREDENTIALS_JSONs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
